# Session Log: 2025-12-19

## Session Goal
Initial simulation boot test and memory system setup.

## Accomplishments
1. Explored Harold codebase structure
2. Identified key files and configurations
3. Successfully booted Isaac Lab simulation (14s boot time)
4. Ran minimal training (1 env, 1 iteration, 24 timesteps)
5. Verified logging infrastructure working
6. Created memory system for cross-session context
7. Created `CLAUDE.md` for automatic session initialization

## Key Findings

### Simulation Infrastructure
- **Boot time**: ~14 seconds (headless mode)
- **Training speed**: ~12 iterations/second (1 env)
- **Logging**: TensorBoard + YAML configs + checkpoints

### Log Structure
```
logs/skrl/harold_direct/YYYY-MM-DD_HH-MM-SS_ppo_torch/
├── checkpoints/       # Agent state files (.pt)
├── params/            # env.yaml, agent.yaml, .pkl files
└── events.out.tfevents.*  # TensorBoard
```

### Hardware (Detected)
- GPU: NVIDIA RTX 4080 (16GB)
- CPU: Intel i7-8700K (6C/12T @ 3.7GHz)
- RAM: 32GB
- OS: Ubuntu 24.04.2 LTS

## Commands Used
```bash
# Boot simulation (minimal test)
cd /home/matteo/Desktop/code_projects/harold
source ~/Desktop/env_isaaclab/bin/activate
python harold_isaac_lab/scripts/skrl/train.py \
  --task=Template-Harold-Direct-flat-terrain-v0 \
  --num_envs 1 \
  --max_iterations 1 \
  --headless
```

## Phase 2: Policy Evaluation (Same Session)

### Key Discovery
**The current best policy (terrain_62) does NOT walk - it stands in place.**

### Evidence
1. Policy playback (500 steps) shows zero forward displacement
2. Training videos show all 4096 robots clustered at spawn positions
3. Robot maintains excellent stability but no locomotion

### Root Cause
Reward structure allows standing as optimal strategy:
- Standing satisfies `upright_reward`, `height_reward`, `rear_support_bonus`
- `progress_forward` weight (80) insufficient to overcome stability rewards
- No penalty for zero velocity

### Next Steps
1. Run EXP-003: Modify rewards to incentivize forward motion
   - Remove `rear_support_bonus` (0.6 → 0.0)
   - Increase `progress_forward_pos` (80 → 150)
   - Consider adding velocity floor penalty

## Notes
- Project has 118+ historical experiments
- Best checkpoint: terrain_62 achieves **standing** not walking
- Memory system now active at `.claude_memory/`
- Critical insight: Must break the "standing equilibrium"
