# Session 15: Early Stopping and Curriculum Experiments

**Date**: 2025-12-24 ~00:00 to ~01:00
**Duration**: ~1 hour
**Experiments**: EXP-059, EXP-060, EXP-061, EXP-062

## Goal
Test early stopping and velocity-based curriculum approaches to prevent mid-training regression observed in Session 14.

## Key Finding
**FAILURE**: All approaches failed. Regression is fundamental to current training setup.
- Robot CAN achieve vx=0.076 (76% of target!) but cannot maintain it
- Peak occurs relative to training progress (80-90%), not absolute iterations
- Velocity decay curriculum prevents forward motion learning entirely

## Technical Implementation

### EXP-059: Early Stopping (50% Duration)
- Config: gait=10.0, 625 iterations (50% of standard)
- Peak: vx=0.076 at 89% of shortened run
- Final: vx=0.027 (regressed)
- Analysis: Peak is proportional to progress, not absolute time

### EXP-060: Very Early Stopping (400 iterations)
- Config: gait=10.0, 400 iterations
- Final: vx=0.026
- Analysis: Learning phases compress proportionally

### EXP-061: Velocity Decay Curriculum (Aggressive)
- Added: `velocity_decay = torch.exp(-vx * 30.0)`
- At vx=0.05: only 22% gait reward remaining
- Result: vx=0.007 (killed by watchdog)
- Analysis: Decay too aggressive

### EXP-062: Softer Velocity Decay
- Changed to: `velocity_decay = torch.exp(-vx * 10.0)`
- At vx=0.05: 61% gait reward remaining
- Result: vx=-0.001 (robot learned to stand still!)
- Analysis: Any decay prevents forward motion learning

## Experiments Summary

| EXP | Config | Final vx | Peak vx | Notes |
|-----|--------|----------|---------|-------|
| 059 | Early stop (50%) | +0.027 | +0.076 | Peak at 89%, then regressed |
| 060 | Very early (400 iter) | +0.026 | - | No improvement |
| 061 | Decay=30 | +0.007 | +0.020 | Killed by watchdog |
| 062 | Decay=10 | -0.001 | - | Learned to stand still |

## Key Findings

1. **Early stopping doesn't prevent regression**
   - Peak occurs at ~80-90% of training regardless of total duration
   - EXP-059 peaked at vx=0.076 at 89%, then regressed by 100%

2. **Velocity decay curriculum FAILS completely**
   - Decaying gait reward as vx increases removes forward motion incentive
   - Robot optimizes for standing still (vxâ‰ˆ0 = max gait reward)
   - Both aggressive (decay=30) and gentle (decay=10) failed

3. **Regression is fundamental to current setup**
   - Robot demonstrates it CAN walk (peak vx=0.076)
   - Policy "forgets" walking behavior during continued training
   - May be PPO-specific (policy gradient instability)

## Configuration Changes

- Reverted velocity_decay to disabled (removed from reward calculation)
- Reverted diagonal_gait_reward from 10.0 back to 5.0 (EXP-056 optimal)

## Files Modified

- `harold_isaac_lab_env.py`: Added then removed velocity_decay
- `harold_isaac_lab_env_cfg.py`: Reset diagonal_gait_reward to 5.0

## Memory Files Updated

- EXPERIMENTS.md: Added Session 15 documentation
- CONTEXT.md: Updated current state and approach status
- NEXT_STEPS.md: Updated priorities (checkpoint selection is now Priority 1)

## Next Session Priorities

1. **Checkpoint selection**: Evaluate agent_13500.pt from EXP-059 (near peak)
2. **Reference motion**: Implement imitation learning with designed gait
3. **PPO tuning**: Increase clip_range to prevent large policy updates

## Summary

Session 15 systematically tested approaches to prevent mid-training regression:
- Early stopping: Failed - peak is relative to progress
- Velocity decay curriculum: Failed - prevents forward learning

The robot CAN achieve vx=0.076 (76% of target) but the policy regresses during continued training. Next focus should be on either:
1. Using mid-training checkpoints (manual checkpoint selection)
2. Fundamentally different approach (reference motion, different algorithm)
