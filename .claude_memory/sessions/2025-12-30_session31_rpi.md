# Session 31: Deployment Stabilization (RPi)

**Date**: 2025-12-30
**Location**: Raspberry Pi 5 (onboard robot)
**Focus**: Fix observation/action pipeline for stable policy outputs

## Problem

Policy outputs were exploding (±100+) causing unstable robot behavior. Root causes:
1. Corrupted lin_vel training statistics
2. Feedback loop divergence in prev_targets
3. Observation mismatch at startup

## Fixes Implemented

### 1. lin_vel Statistics Override
**File**: `inference/harold_controller.py:131-137`

Training statistics showed impossible values:
- Z velocity mean: -44.6 m/s (160 km/h falling!)

Fix: Override to reasonable walking values:
```python
self.running_mean[0:3] = [0.0, 0.0, 0.0]
self.running_var[0:3] = [0.25, 0.25, 0.25]  # std=0.5 m/s
```

### 2. prev_target Blending
**File**: `inference/observation_builder.py:215-243`

prev_target_delta was feeding back into observations, causing exponential divergence.

Fix: Blend actual values with training mean:
```python
prev_targets = 0.1 * actual_delta + 0.9 * training_mean
```

### 3. joint_pos Blending
**Files**: `observation_builder.py:191-196`, `harold_controller.py:255-272`

At startup, joint positions are far from training distribution (shoulders ±3 std deviations).

Fix: Gradual blending during warmup/transition:
- Warmup (0-4.3s): 30% actual, 70% training mean
- Transition (4.3-7.1s): Linear blend to 100%
- After: 100% actual values

## Results

| Metric | Before | After |
|--------|--------|-------|
| Policy range | [-36, +25] | [-8, +14] |
| Max correction | 93° | 41° |
| Stability | Divergent | Bounded |

## Hardware Test

Robot showed walking-like motion on test stand. However, suspended robot means IMU readings don't match simulation (no real acceleration/movement).

## Next Steps (for Desktop Agent)

**Priority**: Validate ONNX against simulation data

1. Create `harold_isaac_lab/scripts/skrl/record_episode.py`
2. Record 200 timesteps of observations + actions from simulation
3. Transfer to RPi: `deployment/validation/sim_episode.json`
4. Run validation: `python validation/validate_onnx_vs_sim.py`

This will confirm whether the ONNX export and observation format are correct.

## Files Modified

- `inference/harold_controller.py` - lin_vel override, joint_pos blending
- `inference/observation_builder.py` - Added blending support
- `test_final.py` - Updated integration test
- `.claude_memory/NEXT_STEPS.md` - Updated priorities
- `validation/validate_onnx_vs_sim.py` - NEW: validation script

## Key Insights

1. **Training statistics can be corrupted** - Always sanity check mean/var values
2. **Feedback loops amplify errors** - Blend with stable reference values
3. **Startup transients matter** - Gradual blending prevents initial spikes
4. **Suspended robot ≠ walking robot** - IMU sees different dynamics on test stand
