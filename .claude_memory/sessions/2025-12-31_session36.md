# Session 36: Pure RL for Velocity-Commanded Walking

**Date**: 2025-12-31 to 2026-01-01
**Goal**: Replace CPG+residual approach with pure RL for velocity-commanded walking (vx, vy, yaw_rate)

## Summary

Implemented pure RL with simplified 11-term reward structure. Ran extensive experiments (EXP-186 to EXP-197) with various reward tuning. **Key finding: Pure RL from scratch plateaus at vx ≈ 0.01 m/s** - robots learn to stand stably but cannot learn walking.

## Key Changes Made

### 1. Reward Structure (harold_isaac_lab_env_cfg.py)
Replaced 30+ reward terms with 11 core terms:

| Term | Weight | Purpose |
|------|--------|---------|
| track_lin_vel_xy | 5.0 | Velocity tracking (exponential kernel, std=0.25) |
| track_ang_vel_z | 2.0 | Yaw rate tracking |
| lin_vel_z | -0.0001 | Penalize vertical bouncing |
| ang_vel_xy | -0.0001 | Penalize roll/pitch rates |
| dof_torques | -0.0001 | Smooth torques |
| dof_acc | -2.5e-7 | Smooth accelerations |
| action_rate | -0.01 | Smooth actions |
| feet_air_time | 1.0 | Encourage stepping |
| undesired_contacts | -1.0 | Penalize body contact |
| upright | 2.0 | Stay upright |
| forward_motion | 3.0 | Direct vx bonus |

### 2. Observation Space
Reduced from 50D to 48D (removed gait phase since no CPG):
- [0:3] Root linear velocity (body frame)
- [3:6] Root angular velocity (body frame)
- [6:9] Projected gravity (body frame)
- [9:21] Joint positions (relative to default)
- [21:33] Joint velocities
- [33:36] Velocity commands [vx, vy, yaw_rate]
- [36:48] Previous actions

### 3. Command Configuration
Final velocity ranges:
- vx: 0.15 to 0.5 m/s (increased from 0.0-0.3)
- vy: -0.15 to 0.15 m/s
- yaw: -0.30 to 0.30 rad/s

## Experiment Results

### Autonomous Session (12 experiments over ~10 hours)

| Exp | Configuration | vx (m/s) | Notes |
|-----|---------------|----------|-------|
| 186 | lin_vel_z=-2.0 | -0.005 | Catastrophic penalty (-24M reward) |
| 187 | (debugging) | - | - |
| 188 | lin_vel_z=-0.05 | +0.008 | Still too high penalty |
| 189 | lin_vel_z=-0.0005 | -0.001 | Penalty balanced |
| 190 | velocity_tracking=5.0 | -0.007 | Added tracking weight |
| 191 | forward_motion=3.0 | **+0.010** | **Best result!** |
| 192 | forward_motion=10.0 | -0.017 | Too aggressive |
| 193 | forward_motion=5.0 | +0.001 | Middle ground failed |
| 194 | 4000 iter (extended) | +0.009 | Extra training didn't help |
| 195 | lin_vel_z=-0.0001 | +0.009 | Reduced penalty |
| 196 | feet_air_time=1.0 | +0.009 | Stepping reward |
| 197 | vx_cmd 0.15-0.5 | +0.009 | Higher commands |

### Key Findings

1. **Forward Motion Weight Sweep**
   - 3.0 is optimal: vx = +0.010
   - 5.0 is worse: vx = +0.001
   - 10.0 is destabilizing: vx = -0.017

2. **lin_vel_z Penalty Scaling**
   - Isaac Lab default -2.0 is catastrophically wrong for Harold
   - Needed 4000x smaller (-0.0005) due to Harold's higher body-frame velocity
   - Further reduction to -0.0001 didn't improve walking

3. **Plateau at vx ≈ 0.01 m/s**
   - All configurations plateau at same velocity
   - Extended training (4000 iter) doesn't break it
   - Increased stepping reward doesn't break it
   - Higher velocity commands don't break it

## Root Cause Analysis

The policy finds a **standing local minimum** that:
- Gives high upright reward (0.97)
- Gives moderate velocity tracking reward (exponential kernel gives ~0.9 even when standing)
- Avoids all penalty terms
- Forward motion bonus too weak to overcome the stable equilibrium

## Recommendations for Future Work

1. **Fine-tune from CPG checkpoint** - The CPG-based policy already walks. Use it as initialization for pure RL.

2. **Curriculum learning** - Start with very low velocity commands (0.05 m/s) and gradually increase.

3. **Different reward formulation**:
   - Sparse terminal reward for distance traveled
   - Asymmetric penalties (penalize backwards, not just non-forward)
   - Gait phase matching reward

4. **Much longer training** - Current 2500 iter (~60 min) may be insufficient. Try 10,000+ iterations.

5. **Reference motion tracking** - Use motion capture or scripted gait as reference trajectory.

## Files Modified

- `harold_isaac_lab_env_cfg.py`: RewardsCfg, CommandCfg, observation_space
- `harold_isaac_lab_env.py`: _reward_keys, _get_observations(), _get_rewards()

## Environment Variables for Pure RL

```bash
HAROLD_CMD_TRACK=1 HAROLD_DYN_CMD=1 python scripts/harold.py train \
  --hypothesis "Pure RL description" \
  --iterations 2500
```

Note: Do NOT set HAROLD_CPG=1 for pure RL mode.
