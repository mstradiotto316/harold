# Session Log: 2025-12-20 (Continuation of 2025-12-19)

## Session Goal
Evaluate current policy, design and run experiments to achieve forward walking.

## Experiments Conducted

### EXP-003: Standing Penalty Approach
- Added standing_penalty=2.0 to penalize low velocity
- Increased forward_reward to 150
- **Result**: FAILED - Robot fell immediately and stayed down
- **Lesson**: Can't just penalize standing

### EXP-004: Balanced Forward Incentive
- Removed standing penalty
- Increased upright_reward: 1.8 → 5.0
- Moderated forward_reward: 150 → 100
- **Result**: FAILED - Robot still fell after ~6000 steps
- **Lesson**: upright_reward=5.0 insufficient

### EXP-005: Fine-tune from terrain_62
- Started from pre-trained checkpoint that can stand
- Same rewards as EXP-004
- **Result**: INCONCLUSIVE - Robot oscillates between standing and falling
- **Lesson**: Even pre-trained stability gets disrupted by forward reward gradient

## Key Technical Insights

### The Fundamental Problem
The reward structure creates **competing gradients**:
- Upright reward: Encourages stability (stand still)
- Forward reward: Encourages motion (which risks falling)

When forward reward gradient > stability gradient, robot falls.
Once fallen, robot can't recover AND may get small rewards from sliding.

### Why Current Approach Fails
1. `upright_sq` modulation isn't enough - gradient still flows
2. No termination on fall - robot learns "fallen" behaviors
3. Forward reward applies even when robot is unstable
4. Robot hasn't learned recovery behaviors

## Recommended Next Approaches

### Approach A: Much Higher Upright Reward
- Set upright_reward = 15-20 (not 5)
- Make stability the DOMINANT objective
- Forward motion becomes secondary optimization

### Approach B: Episode Termination on Fall
- End episode if robot falls (orientation check)
- Robot can't accumulate rewards while fallen
- Forces learning of stable motion only

### Approach C: Explicit Gait Rewards
- Don't just reward velocity
- Reward leg alternation patterns
- Reward feet contact cycling
- This teaches "how to walk" not just "move forward"

### Approach D: Curriculum Learning
1. Phase 1: Standing reward only (achieve stability)
2. Phase 2: Add small forward bias (maintain stability + slight forward)
3. Phase 3: Increase forward bias (walk while stable)

## Files Modified This Session
- `harold_isaac_lab/.../harold_flat/harold_isaac_lab_env_cfg.py` - Reward config
- `harold_isaac_lab/.../harold_flat/harold_isaac_lab_env.py` - Added standing_penalty
- `docs/memory/*` - All memory files updated

## Current Reward Config (after experiments)
```python
progress_forward_pos: 100.0   # Moderate forward
upright_reward: 5.0           # Not enough!
standing_penalty: 0.0         # Disabled
rear_support_bonus: 0.0       # Removed
```

## Next Session Priorities
1. Implement Approach A or B (or combination)
2. Run EXP-006 with much higher upright reward
3. Consider adding termination on fall
4. If still failing, implement gait rewards (Approach C)

## Commands Reference
```bash
# Activate environment
source ~/Desktop/env_isaaclab/bin/activate

# Training with video
python harold_isaac_lab/scripts/skrl/train.py \
  --task=Template-Harold-Direct-flat-terrain-v0 \
  --num_envs 1024 --max_iterations 10000 \
  --video --video_length 300 --video_interval 2000 --headless

# Evaluation with close-up view
python harold_isaac_lab/scripts/skrl/play.py \
  --task=Template-Harold-Direct-flat-terrain-v0 \
  --checkpoint=<path> --video --video_length 300 --num_envs 1 --headless
```

## Session Duration
~2 hours of active experimentation
