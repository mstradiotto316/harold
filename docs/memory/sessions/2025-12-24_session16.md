# Session 16 Log - 2025-12-24 (Autonomous Overnight)

## Session Overview
- **Duration**: ~8 hours autonomous overnight
- **Experiments**: EXP-063 to EXP-068 (6 experiments)
- **Focus**: PPO hyperparameter tuning and forward gating variations
- **Outcome**: All approaches failed; backward drift discovered as stable attractor

## Experiments Run

| EXP | Change | vx | height | Notes |
|-----|--------|-----|--------|-------|
| 063 | Reduced clip_range (0.2→0.1) | +0.024 | 1.26 | Killed by watchdog at ~90% |
| 064 | gait=10 + clip=0.1 | +0.019 | 1.50 | Peak vx=0.029 at 76%, regressed |
| 065 | Extended training (60min) | +0.018 | 1.44 | Peak vx=0.024 at 27%, regressed |
| 066 | gait=10, 4096 envs | -0.040 | 1.96 | Backward drift |
| 067 | Stronger forward gate (scale=50) | -0.018 | 1.89 | Backward drift |
| 068 | Hard forward gate (ReLU-like) | -0.046 | 2.20 | Best height, backward drift |

## Key Discoveries

### 1. Backward Drift is a Stable Attractor
The robot consistently learned to drift backward regardless of forward gating mechanism:
- Sigmoid scale 50: backward drift
- Hard gate (ReLU): backward drift
- The backward optimum appears mechanically easier for the robot

### 2. PPO Clip Range Tuning Failed
Reduced clip_range (0.1 vs 0.2):
- Slowed learning but didn't prevent mid-training regression
- The regression appears to be caused by reward structure, not policy update size

### 3. Height vs Velocity Trade-off
Best height ever achieved (2.20 in EXP-068) came with worst velocity (-0.046):
- Two competing objectives in the reward structure
- Robot optimizes for height when forward motion is difficult

### 4. Memory Watchdog Issues
3 of 6 experiments killed by watchdog (063, 064, 065):
- 6144 envs + overnight runs accumulate memory
- Recommendation: Use 4096 envs for overnight runs

## Code Changes

### During Session (Experimental)
- `ratio_clip`: 0.2 → 0.1 (EXP-063) → 0.2 (EXP-065)
- `diagonal_gait_reward`: 5.0 → 10.0 (EXP-064) → 5.0 (EXP-067)
- `forward_gate`: sigmoid(vx*20) → sigmoid(vx*50) → clamp(vx*20)

### End of Session (Reverted to EXP-056 Best Stable)
- `ratio_clip: 0.2` (standard)
- `diagonal_gait_reward: 5.0` (stable weight)
- `forward_gate: torch.sigmoid(vx * 20.0)` (reverted from hard gate)

## Hypotheses Updated

| ID | Hypothesis | Status |
|----|------------|--------|
| H35 | PPO clip_range adjustment prevents forgetting | DISPROVEN |
| H38 | Backward drift is a stable attractor | CONFIRMED |
| H39 | Hard forward gating prevents backward optimization | DISPROVEN |
| H40 | Explicit backward penalty needed | TO TEST |

## Next Priorities
1. **Checkpoint selection**: Evaluate EXP-059 mid-training checkpoint (vx=0.076)
2. **Explicit backward penalty**: Add penalty for vx < 0 to break drift optimum
3. **Reference motion / imitation learning**: Bypass RL instability

## Memory Files Updated
- CONTEXT.md: Updated current state and approach status
- NEXT_STEPS.md: Added Session 16 summary and updated priorities
- OBSERVATIONS.md: Added Session 16 analysis section
- EXPERIMENTS.md: Added EXP-063 to EXP-068 documentation
