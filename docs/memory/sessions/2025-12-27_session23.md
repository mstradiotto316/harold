# Session 23: Stiffness Optimization for RL Learning

**Date**: 2025-12-27
**Duration**: ~2.5 hours
**Outcome**: Discovered stiffness=600 is better than 400 for RL learning

## Objectives

1. Train RL with stiffness=400 (sim-to-real aligned from Session 22)
2. Test intermediate stiffness values if needed
3. Find optimal stiffness for RL learning while maintaining sim-to-real potential

## Key Findings

### Stiffness=400 Was Too Soft for RL

The "sim-to-real aligned" stiffness=400 from Session 22 caused problems for RL:
- Robot couldn't maintain height (0.74 vs 1.2 threshold)
- Episodes had stability issues (SANITY FAIL during mid-training)
- Final vx=0.024 m/s (24% of target)

### Stiffness=600 Significantly Better

| Metric | Stiffness=400 | Stiffness=600 | Improvement |
|--------|---------------|---------------|-------------|
| Height | 0.74 | 1.54 | +108% |
| vx (final) | 0.024 | 0.027 | +12% |
| vx (peak) | 0.024 | 0.047 | +96% |
| Episode Length | 161 | 442 | +175% |
| Verdict | FAILING | STANDING | - |

### Regression Pattern Confirmed

With stiffness=600:
- Peak vx=0.047 at 39% progress
- Final vx=0.027 at 100% progress
- Regression of 43% from peak to final

This matches the consistent pattern seen in previous sessions.

## Experiments Run

### EXP-109 (stiffness=400)
- **Hypothesis**: RL with stiffness=400 sim-to-real aligned
- **Result**: FAILING
- **Key metrics**: height=0.74, vx=0.024, ep_len=161
- **Note**: Softer stiffness makes learning harder

### EXP-110 (stiffness=600)
- **Hypothesis**: Stiffness=600 middle ground between sim-to-real (400) and learning (1200)
- **Result**: STANDING
- **Key metrics**: height=1.54, vx=0.027, peak_vx=0.047, ep_len=442
- **Note**: Much better learning, proper standing posture

## Technical Details

### Stiffness-Learning Trade-off

There's a tension between:
1. **Sim-to-real fidelity**: Lower stiffness (400) matches real servo "softness"
2. **RL learning ability**: Higher stiffness enables robot to maintain posture during exploration

The real robot (Session 22) walked with soft settings, but RL struggles to learn when actuators are too soft because:
- Robot can't maintain height → exploration is unstable
- Falls more frequently → shorter episodes → less learning signal
- Crouched posture limits gait options

### Current Configuration

After Session 23, harold.py has:
```python
stiffness = 600.0
damping = 45.0
effort_limit = 2.8
```

## Next Steps

1. **Continue stiffness sweep**: Test 800, 1000 to find optimal balance
2. **Early stopping**: Peak vx at ~40% suggests stopping early could preserve better policies
3. **Transfer testing**: Eventually test trained policies on real hardware to validate sim-to-real

## Files Modified

- `harold.py`: stiffness 400→600, damping 40→45
- `docs/memory/NEXT_STEPS.md`: Updated with Session 23 findings
- `docs/memory/sessions/2025-12-27_session23.md`: This file

## Session End State

- Stiffness=600 in harold.py
- Two experiments completed (stiffness=400 and 600)
- Ready to continue stiffness sweep with 800
